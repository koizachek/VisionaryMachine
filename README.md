# VisionaryMachine
A Future Scenario Generator, based on GPT-3's Engine Davinci


Hello, there!

Welcome to this project, I am happy you found it: Originally dedicated as my Master's Thesis at Freie University of Berlin, the >>Visionary Machine<< aims to produce authentic future scenarios, based on a synthetic dataset created with vanilla GPT-3 and an equal-in-size human-made scenario dataset for the years 2030, 2040 and 2050. Both datasets consist of 1056 scenarios. 

What are Scenarios?

No future research methods represent discipline as much as scenario methods. The development of possible future worlds and the description of the way to them provide the basis for decision-making and at the same time space for speculation. The result is described in scenic texts and makes the future narrative, desirable or warns of it. Not least because of the pandemic, possible futures have found their way into the reality of a critical mass - the population received forecasts every day, while economic, political and civil actors speculated about post-pandemic future scenarios. In the course of this, the creation and publication of future scenarios achieved a wide range: An accessible resource that is being collected in large quantities as part of this master’s thesis.

How did I work with the Scenarios?

The scenarios were analyzed with DistilBERT-uncased-emotions before they become (or more specifically, I made them to) the database for the fine-tuning of GPT3.5's Davinci Engine (trying the best model) and Ada Engine (trying the most creative model). My research question deals with the possibility of automatically generating “authentic” visions of the future with the help of artificial intelligence. Text processing is one of the strengths of Narrow Artificial Intelligence, which is very well possible based on current state of the art. 

Why?

From the scenarios generated, I expect to gain insights into the images of the future that human experts have created, because the abstraction by an algorithm shows patterns and quantifies previously uncounted things. This brings content motifs to the surface, connections and clusters are shown. The parameters with which algorithmic models operate are also an important basis for the traceability of the AI. If an algorithm can create images of the future, what would it “imagine”? And is it able to generate other content than the scenarios that the person has developed? Publicly accessible language models refer to various training data from the Internet. The GPT-3 model, which has been accessible since April 2020, has +175 billion parameters and it's training data goes as far as early 2021. With the synthetic dataset on the other hand, I aim to build the foundation of a comparison between the performance measures. 

What could it look like?

<img width="454" alt="grafik" src="https://user-images.githubusercontent.com/85067527/144433365-b5f64754-f82b-414e-a911-762f8ef983f1.png">

A first test run with the opensoource model GPT-Neo reveals the the “imagination” of the AI: Even small changes such as punctuation marks or additional spaces generate images of the future that have little to do with human futures and at the same time make the results opaque and difficult to reconstruct. 

I hope this project will show something slightly more authentic – I will post updates on the project here.
