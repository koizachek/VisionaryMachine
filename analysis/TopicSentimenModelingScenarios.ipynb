{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "860acece",
   "metadata": {},
   "source": [
    "perform topic modeling on a small dataset in Python, you can use the Latent Dirichlet Allocation (LDA) algorithm from the gensim library. LDA is a probabilistic model that assumes that each document in a dataset is a mixture of a small number of topics, and that each word in the document is generated from one of those topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7738194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, pandas as pd\n",
    "from gensim import corpora\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ceb66",
   "metadata": {},
   "source": [
    "## Reinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec408255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file and read the sentences into a list\n",
    "scenario = pd.read_excel('human_finetuned_gpt3_scenarios.xlsx')\n",
    "#scenario = scenario.drop('prompt', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.dropna(subset=['scenario'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da71c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['scenario'] = \\\n",
    "scenario['scenario'].map(lambda x: x.lower())\n",
    "scenario['scenario'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentence(sentence):\n",
    "    words = sentence.split()\n",
    "    unique_words = list(set(words))\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5064f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['scenario_singlewords'] = scenario['scenario'].apply(split_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['scenario_singlewords'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71447ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists in the scenario_singlewords column to tuples\n",
    "scenario['scenario_singlewords'] = scenario['scenario_singlewords'].apply(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244068e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['scenario_singlewords'] = scenario['scenario_singlewords'].apply(lambda x: [re.sub(r'[^\\w\\s]+', '', i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74086ab",
   "metadata": {},
   "source": [
    "## Stopwords und Synonyme filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b0c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stop_words = ['towards','eu','european','europe','still','scenario','increase','increased','due','promoting','fostering','becomes','advanced','reducing','ha', 'use', 'rise', 'led', 'lead', 'form', 'new', 'enable', 'continue', 'widespread']\n",
    "stop_words = stop_words.union(custom_stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word in the list of single words\n",
    "scenario['scenario_singlewords'] = [[lemmatizer.lemmatize(word) for word in word_list] for word_list in scenario['scenario_singlewords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c63ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the single words into a single list\n",
    "all_words = [word for word_list in scenario['scenario_singlewords'] for word in word_list]\n",
    "# Filter out stopwords\n",
    "all_words = [word for word in all_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f24a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords from the list of single words\n",
    "scenario['scenario_singlewords'] = [[word for word in word_list if word not in stop_words] for word_list in scenario['scenario_singlewords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771eb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['scenario_singlewords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(all_words)\n",
    "count_unique_words = len(unique_words)\n",
    "print(count_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6465b97",
   "metadata": {},
   "source": [
    "## Visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9233af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(all_words).most_common(20)\n",
    "# Separate the words and their frequencies into separate lists\n",
    "words, frequencies = zip(*word_counts)\n",
    "words = list(words)\n",
    "frequencies = list(frequencies)\n",
    "\n",
    "# Set seaborn style and plot the bar chart\n",
    "sns.set(style=\"white\", font='monospace')\n",
    "plt.figure(figsize=(12,8), dpi=300)\n",
    "sns.barplot(x=frequencies, y=words, color='black') #45464c #192633 #7a96b3\n",
    "\n",
    "# Add the frequency values as text\n",
    "for i, v in enumerate(frequencies):\n",
    "    plt.text(v + 0.2, i, str(v), color='black')\n",
    "\n",
    "# Set the x-axis limit\n",
    "plt.xlim(0, max(frequencies) + 10)\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('20 Most Used Terms in GPT-3-Made Scenarios', fontsize = 16)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words  (N = 2813)')\n",
    "\n",
    "# Remove spines\n",
    "sns.despine(trim=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c7d8ca",
   "metadata": {},
   "source": [
    "# Sentiment Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words=list(unique_words)\n",
    "#unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "prediction = classifier(unique_words)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario['year']=scenario['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(prediction)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "    new_df = pd.DataFrame(columns=labels)\n",
    "    for i in range(df.shape[0]):\n",
    "        row = {}\n",
    "        for j in range(df.shape[1]):\n",
    "            label = df.iloc[i, j]['label']\n",
    "            score = df.iloc[i, j]['score']\n",
    "            row[label] = score\n",
    "        new_df = new_df.append(row, ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "new_df = transform_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_emotions = pd.concat([scenario['year'],scenario['scenario'], new_df['sadness'], new_df['joy'], new_df['love'], new_df['anger'],new_df['fear'], new_df['surprise']], axis=1)\n",
    "ft_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd56231",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_emotions = ss_emotions.dropna()\n",
    "ft_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eddfc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_emotions.to_csv('gpt3_scenarios_sentiment_analysis_full.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7f489",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476cd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grouped_sentiments(ft_emotions):\n",
    "    # Melt the data frame to have sentiment labels as columns\n",
    "    df_melted = pd.melt(ft_emotions, id_vars=[\"scenario\", \"year\"], value_vars=[\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "    \n",
    "    # Pivot the data so that each sentiment label is a column and each group is a row\n",
    "    pivot_df = df_melted.pivot_table(index='year', columns='variable', values='value', aggfunc='mean')\n",
    "    \n",
    "    # Re-order the columns in the pivot table\n",
    "    pivot_df = pivot_df[[\"anger\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"love\"]]\n",
    "    \n",
    "    # Set the color palette to RdBu\n",
    "    sns.set_palette(\"Greys_r\", n_colors=6)\n",
    "    \n",
    "    # Set the style to whitegrid\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    # Set the font family to monospace\n",
    "    mpl.rcParams['font.family'] = 'monospace'\n",
    "    \n",
    "    # Plot the stacked bar plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 9), dpi=300)\n",
    "    pivot_df.plot(kind='bar', stacked=True, ax=ax)\n",
    "    \n",
    "    # Add labels and titles\n",
    "    ax.set_title(\"Sentiment Scores per Year: GPT-3-Made Scenarios\", fontsize=16)\n",
    "    ax.set_xlabel(\"Scenario of the Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Sentiment Score per Emotion\", fontsize=12)\n",
    "    ax.legend(title='Sentiment Label', fontsize=6)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Remove spines\n",
    "    sns.despine(trim=True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "visualize_grouped_sentiments(ft_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2a292",
   "metadata": {},
   "source": [
    "## Comparing both, Human-Made and Machine-Made Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_emotions = pd.read_csv('../Szenario/humanmade_scenarios_sentiment_analysis_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7487d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_emotions = hs_emotions.drop(hs_emotions[(hs_emotions.year == 2035) | (hs_emotions.year == 2045)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbee21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_emotions = hs_emotions.dropna()\n",
    "hs_emotions = hs_emotions.drop('Unnamed: 0', axis = 1)\n",
    "hs_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec054dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "s35_emotions = pd.read_csv('../Szenario/gpt35_scenarios_sentiment_analysis_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b29fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s35_emotions = s35_emotions.dropna()\n",
    "s35_emotions = s35_emotions.drop('Unnamed: 0', axis = 1)\n",
    "s35_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8eda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_emotions = pd.read_csv('../Szenario/gpt4_scenarios_sentiment_analysis_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37461e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s4_emotions = s4_emotions.dropna()\n",
    "s4_emotions = s4_emotions.drop('Unnamed: 0', axis = 1)\n",
    "s4_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_grouped_sentiments(hs_emotions, ft_emotions, s35_emotions, s4_emotions):\n",
    "    # Melt the data frame to have sentiment labels as columns\n",
    "    df_melted1 = pd.melt(hs_emotions, id_vars=[\"scenario\", \"year\"], value_vars=[\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "    df_melted2 = pd.melt(ft_emotions, id_vars=[\"scenario\", \"year\"], value_vars=[\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "    df_melted3 = pd.melt(s35_emotions, id_vars=[\"scenario\", \"year\"], value_vars=[\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "    df_melted4 = pd.melt(s4_emotions, id_vars=[\"scenario\", \"year\"], value_vars=[\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])\n",
    "    \n",
    "    # Pivot the data so that each sentiment label is a column and each group is a row\n",
    "    pivot_df1 = df_melted1.pivot_table(index='year', columns='variable', values='value', aggfunc='mean')\n",
    "    pivot_df2 = df_melted2.pivot_table(index='year', columns='variable', values='value', aggfunc='mean')\n",
    "    pivot_df3 = df_melted3.pivot_table(index='year', columns='variable', values='value', aggfunc='mean')\n",
    "    pivot_df4 = df_melted4.pivot_table(index='year', columns='variable', values='value', aggfunc='mean')\n",
    "    \n",
    "    # Re-order the columns in the pivot table\n",
    "    pivot_df1 = pivot_df1[[\"anger\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"love\"]]    \n",
    "    pivot_df2 = pivot_df2[[\"anger\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"love\"]]   \n",
    "    pivot_df3 = pivot_df3[[\"anger\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"love\"]]\n",
    "    pivot_df4 = pivot_df4[[\"anger\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"love\"]]\n",
    "    \n",
    "    # Concatenate both pivot tables along the index, and label each data source\n",
    "    pivot_df = pd.concat([pivot_df1, pivot_df2, pivot_df3, pivot_df4], axis=0, keys=[\"Human\", \"GPT-3\", \"GPT-3.5\", \"GPT-4\"])\n",
    "    \n",
    "    # Set the color palette to RdBu\n",
    "    sns.set_palette(\"Greys_r\", n_colors=6)\n",
    "    \n",
    "    # Plot the stacked bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), dpi=300)\n",
    "    pivot_df.plot(kind='bar', stacked=True, ax=ax)\n",
    "    \n",
    "    # Add labels and titles\n",
    "    ax.set_title(\"Sentiment Scores per Year: Human- vs. Machine-Made Scenarios\", fontsize=16)\n",
    "    ax.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Mean Sentiment Score per Emotion\", fontsize=10)\n",
    "    ax.legend(title='Sentiment Label', fontsize=10)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Remove spines\n",
    "    sns.despine(trim=True)\n",
    "    ax.set_xticklabels([label.get_text().replace(\"(\", \"\").replace(\")\", \"\").split('.')[0] for label in ax.get_xticklabels()], rotation=45, ha='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eaa20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_grouped_sentiments(hs_emotions, ft_emotions, s35_emotions, s4_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9e4dc",
   "metadata": {},
   "source": [
    "## T-Test per Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_sadness = hs_emotions['sadness']\n",
    "ft_sadness = ft_emotions['sadness']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_sadness, ft_sadness, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a228fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_sadness = hs_emotions['sadness']\n",
    "ss_sadness = ss_emotions['sadness']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_sadness, ss_sadness, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_sadness = hs_emotions['sadness']\n",
    "s35_sadness = s35_emotions['sadness']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_sadness, s35_sadness, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_fear = hs_emotions['fear']\n",
    "ss_fear = ss_emotions['fear']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_fear, ss_fear, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_fear = hs_emotions['fear']\n",
    "ft_fear = ft_emotions['fear']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_fear, ft_fear, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_fear = hs_emotions['fear']\n",
    "s35_fear = s35_emotions['fear']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_fear, s35_fear, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_anger = hs_emotions['anger']\n",
    "ss_anger = ss_emotions['anger']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_anger, ss_anger, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079ffae",
   "metadata": {},
   "source": [
    "P-value:  0.00000005174194068414625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_anger = hs_emotions['anger']\n",
    "ft_anger = ft_emotions['anger']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_anger, ft_anger, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf5a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_anger = hs_emotions['anger']\n",
    "ss_anger = ss_emotions['anger']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_anger, ss_anger, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3603115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_surprise = hs_emotions['surprise']\n",
    "ss_surprise = ss_emotions['surprise']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_surprise, ss_surprise, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_surprise = hs_emotions['surprise']\n",
    "ft_surprise = ft_emotions['surprise']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_surprise, ft_surprise, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_surprise = hs_emotions['surprise']\n",
    "s35_surprise = s35_emotions['surprise']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_surprise, s35_surprise, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_joy = hs_emotions['joy']\n",
    "ft_joy = ft_emotions['joy']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_joy, ft_joy, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_joy = hs_emotions['joy']\n",
    "ss_joy = ss_emotions['joy']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_joy, ss_joy, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ae106",
   "metadata": {},
   "source": [
    "P-value:  0.0000000000006897228642655168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_joy = hs_emotions['joy']\n",
    "s35_joy = s35_emotions['joy']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_joy, s35_joy, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4957f",
   "metadata": {},
   "source": [
    "P-value:  0.000010639088602384985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51da93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_love = hs_emotions['love']\n",
    "ss_love = ss_emotions['love']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_love, ss_love, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_love = hs_emotions['love']\n",
    "s35_love = s35_emotions['love']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_love, s35_love, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_love = hs_emotions['love']\n",
    "ft_love = ft_emotions['love']\n",
    "\n",
    "t_stat, p_val = ttest_ind(hs_love, ft_love, equal_var=False)\n",
    "\n",
    "print(\"P-value: \", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7212152",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ea0e5",
   "metadata": {},
   "source": [
    "Now that I have a trained model let’s visualize the topics for interpretability. To do so, \n",
    "I’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
    "\n",
    "you can manually select each topic to view its top most frequent and/or “relevant” terms, using \n",
    "different values of the λ parameter. This can help when you’re trying to assign a human \n",
    "interpretable name or “meaning” to each topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59610b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(scenario['scenario_singlewords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f802513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words = remove_stopwords(titles)\n",
    "\n",
    "print(data_words[:1][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3947f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ae943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=20)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3b5e5",
   "metadata": {},
   "source": [
    "## Keyword Extraktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e262723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97129e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = list(itertools.chain.from_iterable(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5344a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ', '.join(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake()\n",
    "r.extract_keywords_from_text(text)\n",
    "keywords = r.get_ranked_phrases()\n",
    "word_counts = Counter(keywords)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3b3a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22808b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f466e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
